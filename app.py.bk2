import streamlit as st
import os
import sys
import socketserver

if not hasattr(socketserver, "UnixStreamServer"):
    socketserver.UnixStreamServer = socketserver.TCPServer

from config_ip import get_windows_ip

SPARK_PATH = "C:/spark/spark-3.4.1-bin-hadoop3"
JAVA_PATH = "C:/Progra~1/Java/jdk-11"
HADOOP_PATH = "C:/spark/spark-3.4.1-bin-hadoop3/hadoop"
WINDOWS_IP = get_windows_ip()

os.environ['SPARK_HOME'] = SPARK_PATH
os.environ['JAVA_HOME'] = JAVA_PATH
os.environ['HADOOP_HOME'] = HADOOP_PATH
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.10'
os.environ["HADOOP_USER_NAME"] = "ubt_trgmai"

sys.path.insert(0, os.path.join(SPARK_PATH, "python"))
sys.path.insert(0, os.path.join(SPARK_PATH, "python", "lib", "py4j-0.10.9.7-src.zip"))

from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from tabs import intro, inference, training

@st.cache_resource
def init_spark():
    conf = SparkConf() \
        .setAppName("Marketing_App_UI") \
        .setMaster(f"spark://{WINDOWS_IP}:7077") \
        .set("spark.driver.host", WINDOWS_IP) \
        .set("spark.driver.bindAddress", WINDOWS_IP) \
        .set("spark.executor.memory", "1g") \
        .set("spark.cores.max", "2") \
        .set("spark.rpc.message.maxSize", "1024") \
        .set("spark.kryoserializer.buffer.max", "1024m")
    return SparkSession.builder.config(conf=conf).getOrCreate()

st.set_page_config(page_title="Big Data Marketing", layout="wide", page_icon="ğŸ›ï¸")

# Header chung
st.title("ğŸ›ï¸ Há»‡ thá»‘ng PhÃ¢n nhÃ³m KhÃ¡ch hÃ ng (Big Data)")
st.caption(f"ğŸš€ **Environment:** Driver (Windows: {WINDOWS_IP}) <--> Worker (WSL/Spark Cluster)")

# Káº¿t ná»‘i Spark
try:
    spark = init_spark()
    st.toast(f"âœ… Spark Connected! Ver: {spark.version}", icon="ğŸ”¥")
except Exception as e:
    st.error(f"âŒ Lá»—i káº¿t ná»‘i Spark: {e}")
    st.stop()

tab1, tab2, tab3 = st.tabs(["â„¹ï¸ Giá»›i thiá»‡u & NhÃ³m", "ğŸ”® Dá»± Ä‘oÃ¡n (Inference)", "ğŸ‹ï¸ Huáº¥n luyá»‡n (Training)"])

with tab1:
    intro.show_page()  # Tab 1: TÄ©nh, khÃ´ng cáº§n Spark session

with tab2:
    inference.show_page(spark) # Tab 2: Cáº§n Spark Ä‘á»ƒ cháº¡y model

with tab3:
    training.show_page(spark)  # Tab 3: Cáº§n Spark Ä‘á»ƒ train