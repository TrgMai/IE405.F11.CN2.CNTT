from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
import json
import os
import shutil
import subprocess
from datetime import datetime

# --- IMPORT C·∫§U H√åNH M√îI TR∆Ø·ªúNG ---
try:
    from config_env import IS_WINDOWS
except ImportError:
    import sys
    IS_WINDOWS = sys.platform.startswith('win')

if IS_WINDOWS:
    from config_ip import get_wsl_ip

# --- 0. HELPER: T·ª∞ ƒê·ªòNG D√í T√åM HDFS ---
def detect_hdfs_path():
    if not IS_WINDOWS:
        return None

    # C√°ch 1: Th·ª≠ l·ªánh 'which hdfs'
    try:
        cmd = ["wsl", "bash", "-l", "-c", "which hdfs"]
        path = subprocess.check_output(cmd).decode("utf-8").strip()
        if path and "hdfs" in path:
            return path
    except:
        pass

    # C√°ch 2: Qu√©t t√¨m file trong th∆∞ m·ª•c Home
    try:
        find_cmd = "find ~/ -name hdfs -type f -path '*/bin/hdfs' 2>/dev/null | head -n 1"
        full_cmd = ["wsl", "bash", "-l", "-c", find_cmd]
        path = subprocess.check_output(full_cmd).decode("utf-8").strip()
        if path:
            return path
    except:
        pass

    return "hdfs"

# --- 1. PIPELINE FACTORY ---
def build_pipeline(k):
    assembler = VectorAssembler(inputCols=["AnnualIncome", "SpendingScore"], outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=True)
    kmeans = KMeans(k=k, seed=42, featuresCol="features", predictionCol="prediction")
    pipeline = Pipeline(stages=[assembler, scaler, kmeans])
    return pipeline

# --- 2. MARKETING STRATEGY ---
def get_marketing_strategy(income, score):
    if income > 70 and score > 60:
        return "VIP", "Thu nh·∫≠p CAO - Chi ti√™u CAO", "üëë ChƒÉm s√≥c ƒë·∫∑c bi·ªát, upsell h√†ng hi·ªáu."
    elif income > 70 and score < 40:
        return "Ti·ªÅm nƒÉng", "Thu nh·∫≠p CAO - Chi ti√™u TH·∫§P", "üíº G·ª£i √Ω s·∫£n ph·∫©m ch·∫•t l∆∞·ª£ng, k√≠ch c·∫ßu."
    elif income < 40 and score > 60:
        return "R·ªßi ro", "Thu nh·∫≠p TH·∫§P - Chi ti√™u CAO", "‚ö†Ô∏è Gi·ªõi thi·ªáu tr·∫£ g√≥p, khuy·∫øn m√£i gi√° r·∫ª."
    elif income < 40 and score < 40:
        return "Ti·∫øt ki·ªám", "Thu nh·∫≠p TH·∫§P - Chi ti√™u TH·∫§P", "üí∞ G·ª≠i voucher gi·∫£m gi√°, h√†ng thi·∫øt y·∫øu."
    else:
        return "Ti√™u chu·∫©n", "Kh√°ch h√†ng Trung b√¨nh", "üìß Duy tr√¨ t∆∞∆°ng t√°c ƒë·ªãnh k·ª≥."

# --- 3. HELPER: CONVERT PATH WINDOWS -> WSL ---
def windows_to_wsl_path(windows_path):
    if not IS_WINDOWS:
        return windows_path # Gi·ªØ nguy√™n n·∫øu l√† Linux
        
    path = os.path.abspath(windows_path).replace("\\", "/")
    if len(path) > 1 and path[1] == ":":
        drive_letter = path[0].lower() # L·∫•y ch·ªØ c√°i ·ªï ƒëƒ©a (c, d, e...)
        rest_of_path = path[2:]        # L·∫•y ph·∫ßn c√≤n l·∫°i
        return f"/mnt/{drive_letter}{rest_of_path}"
    return path 

# --- 4. HYBRID SAVE (QUAN TR·ªåNG: T·ª∞ ƒê·ªòNG CH·ªåN LOGIC L∆ØU) ---
def save_model_hybrid(model, model_name, k, source, local_root="models"):
    
    # T·∫°o th∆∞ m·ª•c local n·∫øu ch∆∞a c√≥
    if not os.path.exists(local_root):
        os.makedirs(local_root)

    # --- TR∆Ø·ªúNG H·ª¢P 1: WINDOWS ---
    if IS_WINDOWS:
        wsl_ip = get_wsl_ip()
        
        # T·ª± ƒë·ªông d√≤ t√¨m ƒë∆∞·ªùng d·∫´n HDFS
        HDFS_BIN = detect_hdfs_path()
        print(f"üîé Detected HDFS Path: {HDFS_BIN}")
        
        # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n
        hdfs_path = f"hdfs://{wsl_ip}:9000/project/models/{model_name}"
        local_path = os.path.abspath(os.path.join(local_root, model_name))
        wsl_local_path = windows_to_wsl_path(local_path) 
        
        # B∆Ø·ªöC 1: L∆∞u l√™n HDFS (Spark Native Write)
        print(f"üîÑ [1/3] Saving to HDFS: {hdfs_path}...")
        try:
            model.write().overwrite().save(hdfs_path)
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u HDFS: {e}")
            raise e
        
        # B∆Ø·ªöC 2: Copy t·ª´ HDFS v·ªÅ Local Windows (Th√¥ng qua WSL CLI)
        print(f"üîÑ [2/3] Syncing to Local Windows: {local_path}...")
        
        if os.path.exists(local_path):
            try:
                shutil.rmtree(local_path)
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a folder c≈©: {e}")
            
        try:
            bash_cmd = f"'{HDFS_BIN}' dfs -get '{hdfs_path}' '{wsl_local_path}'"
            subprocess.check_call(["wsl", "bash", "-l", "-c", bash_cmd])
            print("‚úÖ Sync Local th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói Sync v·ªÅ Windows: {e}")

        # Metadata c√≥ ƒë∆∞·ªùng d·∫´n HDFS
        meta_paths = { "hdfs": hdfs_path, "local": local_path }

    # --- TR∆Ø·ªúNG H·ª¢P 2: LINUX/CLOUD ---
    else:
        print(f"‚òÅÔ∏è Detect Linux Environment. Saving locally to {local_root}...")
        local_path = os.path.join(local_root, model_name)
        
        # X√≥a model c≈© n·∫øu c√≥
        if os.path.exists(local_path):
            shutil.rmtree(local_path)
            
        # L∆∞u tr·ª±c ti·∫øp b·∫±ng Spark (Local FS)
        model.write().overwrite().save(local_path)
        
        # Metadata kh√¥ng c√≥ HDFS
        meta_paths = { "hdfs": None, "local": local_path }

    # --- B∆Ø·ªöC 3: L∆ØU METADATA JSON ---
    meta = {
        "name": model_name,
        "k": k,
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source": source,
        "paths": meta_paths
    }
    
    meta_path = os.path.join(local_root, f"{model_name}_meta.json")
    with open(meta_path, "w", encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=4)
        
    return local_path

# --- 5. SMART LOAD ---
def load_model_smart(model_name, meta=None):
    local_path = os.path.abspath(os.path.join("models", model_name))
    
    # --- ∆Øu ti√™n 1: Load t·ª´ Local (Windows Folder ho·∫∑c Cloud Folder) ---
    if os.path.exists(local_path):
        try:
            # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n cho Spark
            if IS_WINDOWS:
                # Windows c·∫ßn: file:///C:/path/to/model
                uri_path = "file:///" + local_path.replace("\\", "/").lstrip("/")
            else:
                # Linux c·∫ßn: file:///path/to/model
                uri_path = "file://" + local_path
                
            print(f"üìÇ ƒêang load t·ª´ LOCAL: {uri_path}")
            model = PipelineModel.load(uri_path)
            return model, "Local Storage"
        except Exception as e:
            print(f"‚ö†Ô∏è Load Local th·∫•t b·∫°i ({e}). ƒêang th·ª≠ ngu·ªìn kh√°c...")
    
    # --- ∆Øu ti√™n 2: Load t·ª´ HDFS ---
    if IS_WINDOWS:
        if meta and "paths" in meta and meta["paths"]["hdfs"]:
            hdfs_path = meta["paths"]["hdfs"]
        else:
            wsl_ip = get_wsl_ip()
            hdfs_path = f"hdfs://{wsl_ip}:9000/project/models/{model_name}"

        try:
            print(f"‚òÅÔ∏è ƒêang load t·ª´ HDFS: {hdfs_path}")
            model = PipelineModel.load(hdfs_path)
            return model, "HDFS Cluster"
        except Exception as e:
            pass

    raise Exception(f"‚ùå Kh√¥ng th·ªÉ load model '{model_name}' t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o.")

# --- 6. LOAD AVAILABLE MODELS ---
def load_available_models(path="models"):
    models = []
    if os.path.exists(path):
        for item in os.listdir(path):
            if item.endswith("_meta.json"):
                try:
                    with open(os.path.join(path, item), "r", encoding='utf-8') as f:
                        models.append(json.load(f))
                except: continue
    return models